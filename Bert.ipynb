{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT(Bidirectional Encoder Representations from Transformers)\n",
    "BERT uses birectional transformers for analysis.\n",
    "### Transformers -\n",
    "* Transformers are attention based encoder and decoder type architecture.\n",
    "* On high level the encoder maps an input sequence into an abstract continuous representation that holds all the learned information of that input to decoder then takes our cntinuous representation and step by step generates a single output while also being fed the previous output.\n",
    "* It is found that transformers are much better at handling the long term dependencies between words.For example a question answer you have to remember the text that you have read before.In those cases Transformers are doing a better job then LSTMs.\n",
    "### Let us see the working step by step on this-\n",
    "https://towardsdatascience.com/simple-explanation-of-transformers-in-nlp-da1adfc5d64f\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERT introduced the concept of bidirectionality:\n",
    "Bidirectional means that BERT learns information from both the left and the right side of a token's context during the training phase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERT model is basically a stack of  transformer encoders.\n",
    "There are two model sizes for BERT-\n",
    "* Number of encoders for besrt base is 12 and bert large is 24.\n",
    "* Large model is more accurate as it as more parameters but it requires hi-fi configuration machines with multiple GPUs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERT has an attention layer-\n",
    "* Suppose you have a sentence-\n",
    "Jim is pretty cool.His pranks are excellent.\n",
    "* So attention mechganism will tell there is a high co-relation between Jim and his. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Preprocessing-\n",
    "Before feeding our data to our model we require to do some text preprocessing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokinazation-\n",
    "* We convert our text data to tokens using hugging face library.\n",
    "* It uses the pretrained version of BERT base which is lighter BERT model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERT's special preprocessing requirement-\n",
    "* BERT requires some special tokens which are going to indicate seperation between sentences.\n",
    "* BERT requires sequences of equal length and for this we will do some padding which will append tokens of zero meaning to the end of our sentences.\n",
    "* Bert requires attention masking that is each token that has meaning/every token that is not a padding token should have a value of 1 into the attention mask and 0 if the token is padding token."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Special tokens-\n",
    " * Seperation Token([SEP]) - seperates between sentences.\n",
    " * classification token ([CLS])- It will tell BERt that we are interested in sequence classification.\n",
    " * padding token ([PAD]) - it tells that this token is padding token.\n",
    " * unknown token([UNK]) - It is not recognized by the tokenizer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We use tokenizer.encode_plus() method to do all of these at ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
